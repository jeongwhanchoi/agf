<div align=center>
<h1> Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain</h1>

![GitHub Repo stars](https://img.shields.io/github/stars/hyowonwi/agf)
 [![arXiv](https://img.shields.io/badge/arXiv-2505.08516-b31b1b.svg)](arxiv.org/abs/2505.08516)
 [![alphaXiv](https://img.shields.io/badge/alphaXiv-ff0000.svg)](https://www.alphaxiv.org/abs/2505.08516) 
 [![alphaXiv](https://img.shields.io/badge/IJCAI_2025-cab4a3.svg)](https://www.ijcai.org/proceedings/2025/0730) 

<div>
      <a href="https://scholar.google.co.kr/citations?user=-foMLcAAAAAJ&hl=en" target="_blank"><b>Hyowon Wi</b></a><sup>1</sup>,
      <a href="https://www.jeongwhanchoi.com" target="_blank">Jeongwhan Choi</a><sup>1</sup>,
      <a href="https://sites.google.com/view/noseong" target="_blank">Noseong Park</a><sup>1</sup>,
    <div>
    <sup>1</sup>KAIST
    </div>
</div>
</div>

---

This is the official implementation of our IJCAI 2025 paper "Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain" (AGF).

